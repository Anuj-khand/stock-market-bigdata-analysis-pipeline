**‚öôÔ∏è How to Run the Project**

1Ô∏è‚É£ Start Kafka on EC2

Install Kafka on EC2
Start Zookeeper + Kafka Broker
Create Kafka topic

2Ô∏è‚É£ Run Producer (Local / Laptop)
Bash
cd producer
pip install -r requirements.txt
python kafka_producer_realtime.py

3Ô∏è‚É£ Run Consumer (EC2) ‚Üí Store data in S3
Copy code
Bash
cd consumer
pip install -r requirements.txt
python kafka_consumer_to_s3.py

4Ô∏è‚É£ Run Glue Crawler
AWS Glue crawler scans the S3 data
Creates a table in AWS Glue Data Catalog
5Ô∏è‚É£ Query Data in Athena
Example:
Copy code
Sql
SELECT *
FROM stock_bigdata_db."1768239694912_json"
LIMIT 10;
6Ô∏è‚É£ QuickSight Dashboard
Created a QuickSight dashboard with:
KPI cards (Total Records, Total Volume, Avg Close, Highest Price)
Stock closing price trend over time
Volume trend over time
Stock-wise comparisons
üìå Dashboard Preview:
ÔøΩ
üìä Key Analytics Performed
Total stock events ingested
Average closing price by stock symbol
Total traded volume by stock
High/Low volatility comparison
Time-series trends for stock movement
üí∞ Cost Note
After pipeline completion, EC2 can be stopped to avoid additional costs.
S3 + Glue + Athena metadata will remain available for querying and dashboards.
