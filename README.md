# ğŸ“ˆ Stock Market Real-Time ETL Pipeline using Kafka + AWS

This project simulates a *real-time stock market streaming pipeline* using *Apache Kafka* and AWS services.  
Stock market events are produced continuously, streamed through Kafka, stored in an *Amazon S3 data lake, cataloged using **AWS Glue, queried with **Amazon Athena, and visualized using **Amazon QuickSight*.

---

## ğŸš€ Project Objective
To build an end-to-end *big data streaming pipeline* that demonstrates real-time ingestion, storage, cataloging, querying, and dashboarding.

---

## ğŸ—ï¸ Architecture
Producer â†’ Kafka (EC2) â†’ Consumer â†’ S3 â†’ Glue Crawler â†’ Athena â†’ QuickSight Dashboard

ğŸ“Œ Architecture Diagram:  
![Architecture](architecture/architecture.png)

---

## ğŸ› ï¸ Tech Stack
- *Python*
- *Apache Kafka*
- *AWS EC2*
- *Amazon S3*
- *AWS Glue (Crawler + Data Catalog)*
- *Amazon Athena*
- *Amazon QuickSight*

---

## ğŸ“‚ Repository Structure

```bash
.
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ kafka_producer_realtime.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.example.env
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ consumer/
â”‚   â”œâ”€â”€ kafka_consumer_to_s3.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.example.env
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ glue/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ crawler_config.md
â”‚   â”œâ”€â”€ table_schema.md
â”‚   â”œâ”€â”€ permissions.md
â”‚   â””â”€â”€ athena_queries.sql
â”‚
â””â”€â”€ architecture/
    â”œâ”€â”€ architecture.png
    â””â”€â”€ dashboard_preview.png



**Sample Kafka Event (JSON)**
Json
{
  "symbol": "AAPL",
  "open": 179.0,
  "high": 180.09,
  "low": 178.36,
  "close": 178.85,
  "volume": 218029,
  "event_time": "2026-01-12T17:41:44.425Z"
}
**âš™ï¸ How to Run the Project**
1ï¸âƒ£** Start Kafka on EC2**
Install Kafka on EC2
Start Zookeeper + Kafka Broker
Create Kafka topic
2ï¸âƒ£ **Run Producer (Local / Laptop)**
Copy code
Bash
cd producer
pip install -r requirements.txt
python kafka_producer_realtime.py
3ï¸âƒ£ **Run Consumer (EC2) â†’ Store data in S3**
Copy code
Bash
cd consumer
pip install -r requirements.txt
python kafka_consumer_to_s3.py
4ï¸âƒ£ **Run Glue Crawler**
AWS Glue crawler scans the S3 data
Creates a table in AWS Glue Data Catalog
5ï¸âƒ£ **Query Data in Athena**
Example:
Sql
SELECT *
FROM stock_bigdata_db.stock_market_events
LIMIT 10;
6ï¸âƒ£** QuickSight Dashboard**
Created a QuickSight dashboard with:
KPI cards (Total Records, Total Volume, Avg Close, Highest Price)
Stock closing price trend over time
Volume trend over time
Stock-wise comparisons

ğŸ“Œ **Dashboard Preview:**

ğŸ“Š **Key Analytics Performed**
Total stock events ingested
Average closing price by stock symbol
Total traded volume by stock
High/Low volatility comparison
Time-series trends for stock movement
ğŸ’°** Cost Note**
After pipeline completion, EC2 can be stopped to avoid additional costs.
S3 + Glue + Athena metadata will remain available for querying and dashboards.
